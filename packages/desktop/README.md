# Lighthouse Insight Assistant - Desktop POC

Electron desktop app for LLM-powered real-time career profile editing with **network insights**.

## Overview

This POC demonstrates:
1. LLM-assisted profile editing with network-aware suggestions
2. Real-time AI insights using professional network context
3. Career path recommendations based on network patterns

## Features

- üéØ **Intent-based editing** - Choose what to work on (job, education, project, insights)
- ü§ñ **Real-time AI suggestions** - Get contextual feedback as you type (800ms debounce)
- üåê **Network-aware insights** - LLM uses your professional network context for better suggestions
- üì∏ **Screenshot Processing** ‚ú® NEW - Upload screenshots for OCR and automatic insight generation (Ollama only)
- üîí **Privacy-First Local LLMs** - Run entirely on-device with Ollama (no cloud APIs)
- üíæ **Session-only editing** - Changes not persisted (POC only)
- üé® **Clean UI** - Three-screen flow: Profile ‚Üí Intent ‚Üí Editor

## Network Insights Integration

The app includes **network insights** that provide the LLM with context about:

- Your professional connections (names, roles, companies)
- Common companies and schools in your network
- Overlapping skills with your network
- Career path patterns from successful transitions in your network
- Industry distribution of your connections

This enables the LLM to provide suggestions like:
- "Based on Sarah Chen's transition from Senior to Staff Engineer at Google..."
- "Your network shows strong expertise in Microservices - consider highlighting this"
- "Common career path in your network: IC ‚Üí Manager in 4-6 years"

## Tech Stack

- **Electron** 38+ with Forge + Vite
- **React** 18 with TypeScript
- **LLM Options:**
  - **Ollama** - Local models (Llama 3.2, Qwen, Gemma, etc.) with vision support ‚≠ê Recommended
  - **Vercel AI SDK** - OpenAI integration (GPT-4o-mini)
  - **Anthropic Claude** (Haiku)
- **Sonner** for toast notifications
- **Tailwind CSS** for styling

## Setup

```bash
# Install dependencies (using npm for Electron compatibility)
npm install

# Start development server
npm start
```

## Configuration

### Using Ollama - Local & Privacy-First (Recommended) ‚ú®

**NEW**: Run entirely locally with no cloud APIs required!

```bash
# 1. Pull required models
ollama pull llama3.2:3b           # Text model (fast, 4GB RAM)
ollama pull llava:7b-v1.6-mistral-q4_0   # Vision model for screenshots (4GB RAM)

# 2. Start Ollama
ollama serve

# 3. Configure app (create .env file)
echo "LLM_ADAPTER=ollama" > .env
echo "OLLAMA_TEXT_MODEL=llama3.2:3b" >> .env
echo "OLLAMA_VISION_MODEL=llava:7b-v1.6-mistral-q4_0" >> .env

# 4. Start app
npm start
```

**Benefits:**
- ‚úÖ **100% Free** - No API costs
- ‚úÖ **Privacy-First** - All processing on-device
- ‚úÖ **Offline Capable** - Works without internet
- ‚úÖ **Screenshot Processing** - Vision models for OCR and insight extraction
- ‚úÖ **Model Choice** - Use any compatible Ollama model

**Screenshot Processing (Vision Feature):**
With Ollama, you can now upload screenshots of resumes or documents for automatic:
- Text extraction (OCR)
- Structured data parsing
- Context-aware suggestions based on extracted content

### Using OpenAI

Cloud-based with excellent quality:

```bash
export LLM_ADAPTER=openai
export OPENAI_API_KEY=your_key_here
npm start
```

### Using Claude

```bash
export LLM_ADAPTER=claude
export ANTHROPIC_API_KEY=your_key_here
npm start
```

## Usage

1. **Select Profile** - Click on a profile card
2. **Choose Intent** - Pick editing goal from the intent selector screen
3. **Start Editing** - Type in the textarea
4. **Receive Insights** - Toast notifications with network-aware suggestions appear after 800ms
5. **Stop Session** - Click "Stop Session" to return to profile selection

## Project Structure

```
src/
‚îú‚îÄ‚îÄ main/
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm-service.ts                    # LLM service with adapter support
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ollama-adapter.ts                 # Ollama adapter (text + vision)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ screenshot-processing-service.ts  # Screenshot OCR and insights
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ profile-loader.ts                 # Load user profiles
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ network-insights-service.ts       # Generate network insights
‚îÇ   ‚îî‚îÄ‚îÄ main.ts                               # IPC handlers
‚îú‚îÄ‚îÄ renderer/
‚îÇ   ‚îú‚îÄ‚îÄ App.tsx                     # Main React component
‚îÇ   ‚îî‚îÄ‚îÄ hooks/
‚îÇ       ‚îî‚îÄ‚îÄ useDebounce.ts          # Debounce hook
‚îî‚îÄ‚îÄ shared/
    ‚îî‚îÄ‚îÄ types.ts                    # Shared types including NetworkInsights
```

## Network Insights Types

```typescript
interface NetworkInsights {
  connections: NetworkConnection[]        // Your network connections
  commonCompanies: string[]              // Shared employers
  commonSchools: string[]                // Shared education
  industryDistribution: Record<string, number>  // Industry breakdown
  skillOverlap: string[]                 // Common skills
  careerPaths: CareerPath[]             // Successful career transitions
}

interface CareerPath {
  description: string                    // Career transition pattern
  examplePeople: string[]               // People who took this path
  commonTransitions: string[]           // Steps in the transition
  timeframe: string                     // How long it typically takes
}
```

## Available Ollama Models

### Text Models (for suggestions)

| Model | Size | RAM | Speed | Quality | Command |
|-------|------|-----|-------|---------|---------|
| **llama3.2:3b** ‚≠ê | 3B | 4GB | Very Fast | Good | `ollama pull llama3.2:3b` |
| llama3.2:11b | 11B | 8GB | Fast | Very Good | `ollama pull llama3.2:11b` |
| qwen2.5:7b | 7B | 6GB | Fast | Excellent | `ollama pull qwen2.5:7b` |
| gemma2:9b | 9B | 8GB | Medium | Excellent | `ollama pull gemma2:9b` |
| mistral:7b | 7B | 6GB | Fast | Very Good | `ollama pull mistral:7b` |

### Vision Models (for screenshots)

| Model | Size | RAM | OCR Accuracy | Command |
|-------|------|-----|--------------|---------|
| **llama3.2-vision:11b** ‚≠ê | 11B | 8GB | 85-90% | `ollama pull llama3.2-vision:11b` |
| llama3.2-vision:90b | 90B | 64GB | 95%+ | `ollama pull llama3.2-vision:90b` |
| llava:latest | 7B | 6GB | 80-85% | `ollama pull llava:latest` |
| minicpm-v:latest | 8B | 8GB | 85-90% | `ollama pull minicpm-v:latest` |

**Change models via .env:**
```bash
OLLAMA_TEXT_MODEL=qwen2.5:7b
OLLAMA_VISION_MODEL=llama3.2-vision:11b
```

## LLM Behavior

- Suggestions appear after 800ms of inactivity
- Minimum 10 characters required
- Only shows suggestions with ‚â•80% confidence
- Toast notifications for suggestions and errors
- Retry button for recoverable errors
- **Network insights enhance suggestion quality and personalization**
- **Screenshot processing extracts text and generates 3 tailored suggestions** (Ollama only)

## POC Limitations

- No persistence (session-only editing)
- Mock network data (not connected to real graph database)
- Single profile editing at a time
- No authentication
- No collaboration features
- Screenshot processing requires Ollama (not available with OpenAI/Claude adapters)

## Future Enhancements

### Implemented ‚úÖ
- Local LLM support via Ollama
- Screenshot OCR and processing with vision models
- Multiple adapter support (Ollama/OpenAI/Claude)
- User-configurable model selection

### Planned üöÄ
- UI for screenshot upload and processing
- Batch screenshot processing
- Screenshot history and comparison
- Connect to real network graph (LinkedIn, company directory)
- Real-time network updates
- Compare multiple career paths
- Network visualization
- Personalized insights dashboard
- Export career trajectory analysis
- Fine-tuned local models for career writing

## Development Commands

```bash
npm start          # Start dev server
npm run package    # Package for distribution
npm run make       # Create installers
```

## License

MIT

---

**Status**: POC Complete
**Linear**: LIG-195
**Date**: 2025-10-07
